CONTENT DISCOVERY
=-=-=-=-=-=-=-=-=-
# what is content?
	- Content may be file, pic, vid, backup, or even a website feature, in general we talk about resources that aren't available for public.
# There are 3 ways for content discovery:
		1. Manually
		2. Automated
		3. OSINT
 
1. MANUALLY
------------
- Robots.txt => Is a document file that tells the search engins which pages they are, and this file is not allowed to appear in the search engines results, also it has the allowed dir to show and the dis allowed ones.

- Favicon => when using framework to build a website, the framework came with its icon.
	     if the developer didn't replace it, we can know which framework was used, and exploit
	     it using known CVEs.
# for example
	https://static-labs.tryhackme.cloud/sites/favicon/images/favicon.ico
	you have found this framewark icon

using linux:
	curl https://static-labs.tryhackme.cloud/sites/favicon/images/favicon.ico | md5sum
to get the md5 hash of the icon an search here: https://wiki.owasp.org/index.php/OWASP_favicon_database to know which framework that website uses.

- Sitemap.xml => List of every file the ownere of the website wishs to be listed in the search engine, it also can contan areas of the website are difficult to navigate, or old webpages.

- HTTP Headers => When we make a request to the web server it returns an HTTP Headers. These headers may contain useful info such as the webserver software & the programming language in use.
# we can use curl to send a request using the terminal:
	curl https://example.com -v 
	-v => to veiw the request & response in the terminal.
	
- Framework Stack => After you knew the framework used either from the favicon or from clues in the page source such as comments, you can  locate the framework page and learn more about the software and more other info that may lead to discover more content.


2. OSINT (External)
---------
- Google Hacking/Dorking => some keywords using while searching with google to pick out custom content.
		site:
		inurl:
		intext:
		filetype:
		intitle:
for more:
https://en.wikipedia.org/wiki/Google_hacking
https://www.exploit-db.com/google-hacking-database

- Wappalyzer => Is a tool and extension helps identify what technologies a website uses such as framework, Content Managment System CDN , payment processor and more.

- WayBack Machine => The Internet archive are in there, you can check the old sites, there may be active links on the current website.
links:
https://archive.org/web
http://web.archive.org/cdx/search/cdx?url=target.com%2F*&output=text&fl=original&collapse=urlkey&filter=statuscode%3A200 

- GitHub OSINT => GIT is a version control system 
you can use github dorks to get sensetive information.

- S3 Buckets 
	S3 => Is a service provided by Amazon Web Servers (AWS) to store sensitive information in a the cloud.
S3 Buckets are a storage service provided by Amazon AWS, allowing people to save files and even static website content in the cloud accessible over HTTP and HTTPS. The owner of the files can set access permissions to either make files public, private and even writable. Sometimes these access permissions are incorrectly set and inadvertently allow access to files that shouldn't be available to the public. The format of the S3 buckets is http(s)://{name}.s3.amazonaws.com where {name} is decided by the owner, such as tryhackme-assets.s3.amazonaws.com. S3 buckets can be discovered in many ways, such as finding the URLs in the website's page source, GitHub repositories, or even automating the process. One common automation method is by using the company name followed by common terms such as {name}-assets, {name}-www, {name}-public, {name}-private, etc.


3. AUTOMATED
------------ 
- Automated Discovery => Is the process to use tools to discover content with wordlists
- Content Discovery Wordlists => https://github.com/danielmiessler/SecLists
- Password Cracking Wordlist => https://www.kaggle.com/datasets/wjburns/common-password-list-rockyoutxt

- Content Discovey Tools:
	ffuf => ffuf -u http://10.10.135.170/FUZZ -w common.txt
	dirb => dirb http://10.10.135.170 common.txt
	dirsearch => dirsearch -u http://10.10.135.170 (has internal wordlist)
	gobuster => gobuster dir --url http://10.10.135.170 -w common.txt
dirb => slow
dirsearch => good but not perfect
ffuf & gobuster => The Best
